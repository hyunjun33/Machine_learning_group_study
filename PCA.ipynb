{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처\n",
    "* PCA : \"분산이 큰 축일 수록 정보가 더 많다?\" : https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=hancury&logNo=221215245092\n",
    "* [핵심 머신러닝] Principal Component Analysis (PCA, 주성분 분석) https://www.youtube.com/watch?v=FhQm2Tc8Kic&ab_channel=%E2%80%8D%EA%B9%80%EC%84%B1%EB%B2%94%5B%EC%86%8C%EC%9E%A5%2F%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EA%B3%B5%ED%95%99%EC%97%B0%EA%B5%AC%EC%86%8C%5D\n",
    "* Dimension / 차원 / 차원의 저주 / 차원축소 https://kkokkilkon.tistory.com/127\n",
    "* 차원의 저주  : https://leedakyeong.tistory.com/entry/curse-of-dimensionality-%EC%B0%A8%EC%9B%90%EC%9D%98-%EC%A0%80%EC%A3%BC\n",
    "* 차원, 위키백과 https://ko.wikipedia.org/wiki/%EC%B0%A8%EC%9B%90\n",
    "* 차원 축소와 PCA , https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-9-PCA-Principal-Components-Analysis\n",
    "* 주성분 분석(PCA)의 개념적 이해 : https://everyday-tech.tistory.com/entry/%EC%A3%BC%EC%84%B1%EB%B6%84-%EB%B6%84%EC%84%9DPCA\n",
    "* 주성분 분석: https://skyil.tistory.com/171\n",
    "* 주성분의 개수 : https://techblog-history-younghunjo1.tistory.com/134"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차원과 차원 축소\n",
    "* ### 차원(Dimension)\n",
    "    * 수학에서 공간 내에 있는 점 등의 위치를 나타내기 위해 필요한 축의 개수\n",
    "    <img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Dimension_levels.svg/750px-Dimension_levels.svg.png'>  \n",
    "<br/> \n",
    "    * 데이터에서 차원이라하면 변수의 수라고 이해 (변수가 늘어난다 = 차원이 늘어난다 = 데이터 공간이 커진다)\n",
    "\n",
    "    <img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile23.uf.tistory.com%2Fimage%2F99D7083359F0654824C156'>\n",
    "    <img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile27.uf.tistory.com%2Fimage%2F99DC193359F066D21BB429'>\n",
    "\n",
    "* ### 고차원 데이터\n",
    "    * 변수의 수가 많음 (불필요한 변수 존재)\n",
    "    * 시각적으로 표현하기 어려움\n",
    "    * 계산 복잡도가 증가하여 모델링 비효율적일 가능성이 큼\n",
    "    \n",
    "    * #### 차원의 저주\n",
    "        * 변수의 수가 늘어나 차원이 커지면서 발생하는 문제를 차원의 저주라고 한다.\n",
    "        * 차원이 늘어남에 따라 필요한 데이터 양이 급격하게 증가 (같은 공간의 비율을 설명하기 위해 필요한 데이터 양이 급격히 증가)\n",
    "        * https://leedakyeong.tistory.com/entry/curse-of-dimensionality-%EC%B0%A8%EC%9B%90%EC%9D%98-%EC%A0%80%EC%A3%BC\n",
    "        * 빈 공간이 생겼다는 것은 컴퓨터상으로 0으로 채워졌다는 뜻이고 정보가 없는것이므로 모델 돌릴때 성능이 저하(overfitting발생)  \n",
    "\n",
    "    * #### 다중공선성 문제\n",
    "        * 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어짐\n",
    "            * 피처가 많을 경우 개별 피처간에 상관관계가 높을 가능성이 커서 선형 모델에서는 다중 공선성 문제로 모델의 예측 성능이 저하\n",
    "        <br/>            \n",
    "        \n",
    "### => 중요한 변수만을 선택하여 모델링하는 차원축소가 필요\n",
    "\n",
    "* ### 차원축소 \n",
    "    * 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것 (차원의 수를 줄이는것, 변수의 수를 줄이는 것)\n",
    "* ### 차원 축소의 목적\n",
    "\n",
    "    * 다차원의 피처를 차원 축소해 피처수를 줄이면 더 직관적으로 데이터를 해석 가능(시각화 용이성)\n",
    "    * 노이즈 제거\n",
    "    * 메모리 절약\n",
    "    * 퍼포먼스 향상\n",
    "* ### 차원 축소의 방법\n",
    "    * #### 피처 선택(feature selection) : 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고 데이터 특징을 잘 나타내는 주요 피처만 선택\n",
    "        * 변수 $ x_1, x_2,...., x_{100} => x_1, x_5, x_{25}$ 선택\n",
    "        * 장점 : 선택한 변수 해석 용이\n",
    "        * 단점 : 변수간 상간관계 고려 어려움\n",
    "    * #### 피처 추출(feature extraction) : 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것(변수 추출을 위해 주로 사용되는 방법이 PCA)\n",
    "        * 기존 변수의 선형결합을 통해 새로운 변수 정의 $z_1 = f(x_1, ... ,x_{100}) $\n",
    "        * 장점 : 변수간 상관관계 고려, 일반적으로 변수의 개수를 많이 줄일 수 있음\n",
    "        * 단점 : 추출된 변수의 해석이 어려움\n",
    "* 차원 축소 알고리즘 : PCA(Principal Component Analysis), SVD(Singular Value Decomposition), NMF(Non-Negative Matrix Factorization)\n",
    "* 차원 축소 사용되는 영역(고차원 데이터)\n",
    "    * 이미지 데이터 : 이미지 픽셀 하나하나가 x변수, 고해상도 이미지는 x 변수가 굉장히 많음\n",
    "    * 시그널의 형태 : 선들이 굉장히 많지만 선들은 많은 점들의 연속이므로 점 하나하나가 x 변수\n",
    "    * 텍스트 데이터\n",
    "    * 네트워크 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA(Principal Component Analysis) : 주성분 분석\n",
    "* 대표적인 차원 축소 기법\n",
    "* 가장 높은 분산을 가지는(원 데이터의 분산을 최대한 보존하는) 데이터의 축을 찾아 이 축으로 차원 축소(사영 Projection)\n",
    "\n",
    "* example (3차원> 2차원> 1차원)\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9a816735-d151-414d-9042-b40c525d8c25/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210820%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210820T052714Z&X-Amz-Expires=86400&X-Amz-Signature=a7bd2bd5c38a9f18637e491c8862a0bba8ed1b8a92c03a36aa7939acb6fa7c82&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>\n",
    "\n",
    "* 주성분 분석으로 차원을 줄임 : 원데이터의 차원을 줄였지만 패턴을 잘 보전함\n",
    "\n",
    "* ### 분산 = 넓게 퍼진 정도 : 데이터 분포를 유지\n",
    "2차원 dataset을 1차원 축에 사영\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a6f7458f-8f78-442a-a95e-4b0036f72228/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210820%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210820T054638Z&X-Amz-Expires=86400&X-Amz-Signature=f2680c6f3209e7faeb5100af93bfe15fc4a396d0705be1e21808229c150be624&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>\n",
    "1번 축이 좋다. 원데이터 분산을 가장 잘 보존함\n",
    "\n",
    "\n",
    "* 분산과 PCA : https://everyday-tech.tistory.com/entry/%EC%A3%BC%EC%84%B1%EB%B6%84-%EB%B6%84%EC%84%9DPCA\n",
    "\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/35d7e503-a3bc-4d74-80df-5ac8cc636c9d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210820%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210820T052603Z&X-Amz-Expires=86400&X-Amz-Signature=8dec884fa08e3d638dfea8899335eeb3b9bc77b0756451b1b39ce6730f1831dd&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>\n",
    "\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4e4100ae-4b81-4e50-a1b5-c99a30927271/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210820%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210820T053246Z&X-Amz-Expires=86400&X-Amz-Signature=efbff1b366aa5769c3fd84eb86e913748fc05e18c880300267944d70cd5f735c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>\n",
    "\n",
    "* Z는 모든 X들의 선형 결합, 알파 값이 달라질 뿐\n",
    "* Z가 주성분\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA 수리적 알고리즘\n",
    "* 입력데이터의 공분산 행렬(Covariance Matrix)을 고유값 분해하고, 이렇게 구한 고유벡터에 입력 데이터를 선형 변환 하는것\n",
    "\n",
    "### 공분산\n",
    "* 한 변수간의 변동을 분산, 두 변수 간의 변동은 공분산\n",
    "\n",
    "### 고유벡터\n",
    "* 행렬 A를 곱하더라도 방향이 변하지 않고 크기만 변하는 벡터\n",
    "* $Ax = ax$\n",
    "\n",
    "### 고유값\n",
    "* 해당 고유 벡터의 크기 (화살표의 길이) \n",
    "\n",
    "### 선형변환\n",
    "* 특정 벡터에 행렬 A를 곱해 새로운 벡터로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA 과정\n",
    "1) 수많은 화살표 들 중, 데이터 들을 화살표에 내렸을 때,  데이터가 최대한 안 겹치게, 멀리 퍼지게 하는 길이가 긴 화살표 찾기  \n",
    "2) 거기에 데이터들을 투영  \n",
    "3) 만약 또 하나의 화살표 만들 때 축 끼리는 직각이 되어야 함 (최대한 데이터가 겹치지 않도록) \n",
    "\n",
    "==> 이걸 선형대수학에서의 워딩으로 해석하자면  \n",
    "1) 평균이 0이 되도록 Normalize : 평균을 구해 데이터에 평균만큼 빼면 전체 평균 0  \n",
    "$ x_i = x_i - \\mu $  \n",
    "2) 공분산 행렬에서 고유 벡터/고유값을 구하고  \n",
    "    * 공분산 행렬  \n",
    "    $$C = P\\sum^\\ P^{T}$$  \n",
    "    P : 직교행렬- 행벡터와 열벡터가 유클리드 공간의 정규 직교 기저를 이루는 실수 행렬  \n",
    "    $ \\sum$: 정방행렬  \n",
    "    $ P^{T}$ :전치행렬   \n",
    "    \n",
    "2) 가장 분산이 큰 방향을 가진 고유벡터(e1) 에 입력데이터를 선형변환  \n",
    "3) 고유벡터(e1) 과 직교하며, e1 다음으로 분산이 큰 e2 고유벡터에 또 선형변환. \n",
    "\n",
    "참조 : https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주성분 개수 선정\n",
    "* 고유값 감소율이 유의미하게 낮아지는 Elbow Point에 해당하는 주성분 선택(scree plot)\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2e5807b9-2304-4d75-9d11-65501b565151/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210820%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210820T083142Z&X-Amz-Expires=86400&X-Amz-Signature=08f7c937c85a73c6996756e3d48e671674edb4f3c632640004836398e59582ae&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>\n",
    "\n",
    "* 누적 기여율이 85% 이상인 주성분까지 선택\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a2196065-3c1d-4c08-a6df-6ff27c94ad6f/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210820%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210820T082928Z&X-Amz-Expires=86400&X-Amz-Signature=3bf208cc496407302103cfefa11ab3bf9b3befafbb41844f8d74dfacaccbf8c2&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주성분 분석의 한계\n",
    "* 가우시안 분포가 아닌 자료에 적용하기 어려움\n",
    "    * 데이터의 분포가 가우시안 분포임을 가정하기에 가우시안이 아니거나 다중 가우시안 분포인 경우 적용하기 어려움\n",
    "\n",
    "* 분류 문제를 위해 디자인 되지 않음\n",
    "    * 무조건 공분산을 올리는 방향으로 축을 바꾸기 때문에 분류 성능 향상을 보장하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
